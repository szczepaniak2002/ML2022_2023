{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Understanding data and fixing data types\n",
    "# \n",
    "# \n",
    "# dataset:\n",
    "# https://www.kaggle.com/datasets/parisrohan/credit-score-classification\n",
    "# \n",
    "# useful articles:\n",
    "# https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4\n",
    "# https://medium.com/omarelgabrys-blog/statistics-probability-exploratory-data-analysis-714f361b43d1#a7e5\n",
    "# https://cxl.com/blog/outliers/#h-3-change-the-value-of-outliers\n",
    "# \n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# importing needed packages\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "#importing data\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "# test_df = pd.read_csv(\"test.csv\") not needed to this project, we split train_df data\n",
    "\n",
    "# checking proportions of the train/test data\n",
    "# print(df.shape)\n",
    "# print(test_df.shape)\n",
    "\n",
    "# test data does not include target column, it will not be used\n",
    "# let's keep initial df, as working df could will be modified\n",
    "import copy\n",
    "og_train_df = copy.deepcopy(df)\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# splitting the data \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Podział na zbiory treningowy i testowy\n",
    "train_test_df, validation_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "# podział na test i walidację\n",
    "train_df, test_df = train_test_split(train_test_df, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = train_df.Credit_Score\n",
    "y_test = test_df.Credit_Score\n",
    "y_val = validation_df.Credit_Score\n",
    "\n",
    "train_df.drop('Credit_Score', axis=1)\n",
    "test_df = test_df.drop('Credit_Score', axis=1)\n",
    "validation_df = validation_df.drop('Credit_Score', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Wyświetlenie rozmiarów zbiorów treningowego i testowego\n",
    "# print(\"Rozmiar zbioru walidacyjnego:\", len(validation_df))\n",
    "# print(\"Rozmiar zbioru treningowego:\", len(train_df))\n",
    "# print(\"Rozmiar zbioru testowego:\", len(test_df))\n",
    "\n",
    "\n",
    "# display(train_df)\n",
    "# display(test_df)\n",
    "# display(validation_df)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "df[df.duplicated()].drop_duplicates()\n",
    "# there are no duplicates\n",
    "df.drop(['ID', 'Customer_ID'], axis = 'columns')[df.duplicated()].drop_duplicates()\n",
    "# no duplicates in whole data set\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# # checking number of non-null values and data types of culumns\n",
    "# train_df.info()\n",
    "# test_df.info()\n",
    "# validation_df.info()\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# train_df['Credit_Score'].value_counts()\n",
    "# dane_do_wykresu = [['Poor',28998],['Standard',53174],['Good',17828]]\n",
    "# df_wykres = pd.DataFrame(dane_do_wykresu)\n",
    "# df_wykres.columns = 'Credit Score', 'Count'\n",
    "# # df_wykres\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# # utworzenie półkołowego wykresu danych z kolumny\n",
    "# ax.pie(df_wykres['Count'], labels=df_wykres['Credit Score'], startangle=90, counterclock=False, autopct='%1.1f%%')\n",
    "\n",
    "# # dodanie tytułu do wykresu\n",
    "# ax.set_title('Credit Score')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# checking statistics for numeric data\n",
    "# train_df.describe().T\n",
    "\n",
    "#DONE seeing possible outliers in: Num_Bank_Accounts, Num_Credit_Card, Interest_Rate, Num_Credit_Inquiries, Total_EMI_per_month\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# checking percentage of null rows in columns\n",
    "# print(\"Percent of nulls in train_df\")\n",
    "# print(train_df.isna().mean()*100)\n",
    "\n",
    "# print(\"Percent of nulls in test_df\")\n",
    "# print(test_df.isna().mean()*100)\n",
    "\n",
    "# print(\"Percent of nulls in validation_df\")\n",
    "# print(validation_df.isna().mean()*100)\n",
    "\n",
    "# conclusions:\n",
    "# percent of nulls in each dataframe is similiar for each column\n",
    "# we will remove all nulls with the same method\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Load your dataset\n",
    "df = copy.deepcopy(train_df)\n",
    "\n",
    "# Calculate the percentage of NaN values in each column\n",
    "nan_percentages = df.isna().mean() * 100\n",
    "nan_percentages = nan_percentages[nan_percentages != 0]\n",
    "\n",
    "\n",
    "# Define a color gradient based on the percentage of NaN values\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list('mycmap', ['green', 'yellow', 'red'])\n",
    "\n",
    "# plt.bar(nan_percentages.index, nan_percentages.values, color=cmap(nan_percentages.values / 100))\n",
    "# plt.xlabel('Column')\n",
    "# plt.ylabel('% of NaN values')\n",
    "# plt.title('Percentage of NaN values in each column')\n",
    "\n",
    "# # Rotate the x-axis labels to be upright\n",
    "# plt.xticks(rotation=90)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# checking significant values in columns\n",
    "\n",
    "# for col in train_df.columns:\n",
    "#     print(\"Column name: \" + col + \", dtype: \" + str(train_df[col].dtype))\n",
    "#     print(train_df[col].value_counts(dropna = False))\n",
    "#   print(\"\\n\")\n",
    "\n",
    "# Conclusions:\n",
    "#DONE we could remove ID, Customer_ID, Name, SSN (social security number of a person)\n",
    "#DONE Age above 120 should be replaced with other value\n",
    "#DONE Annual_Income, Num_Bank_Accounts, Num_Credit_Card, Interest_Rate, Num_of_Loan, Num_of_Delayed_Payment, Num_Credit_Inquiries, Monthly_Balance might have outliers\n",
    "# Type_of_Loan has values separated by coma, that needs to be modified\n",
    "\n",
    "#DONE Delay_from_due_date consider negative values (IT'S FINE)\n",
    "#DONE Changed_Credit_Limit underscore (\"_\") propably means 0 (limit didn't change)\n",
    "#DONE Credit_Mix underscore (\"_\") should be changed\n",
    "#DONE Outstanding_Debt, Amount_invested_monthly  underscore (\"_\") should be removed\n",
    "#DONE Credit_History_Age has to be modified to numeric instead of described verbally data\n",
    "#DONE Payment_Behaviour \"!@9#%8\" has to be changed\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "# train_df.info()\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# Credit_Score - change to numeric\n",
    "numeric_mapping_dict = {'Standard': 0, 'Poor': -1, 'Good': 1}\n",
    "# train_df['Credit_Score'] = train_df['Credit_Score'].map(numeric_mapping_dict)\n",
    "# train_df['Credit_Score'] = train_df.Credit_Score.astype(int)\n",
    "\n",
    "y_train = y_train.map(numeric_mapping_dict)\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "y_test = y_test.map(numeric_mapping_dict)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "y_val = y_val.map(numeric_mapping_dict)\n",
    "y_val = y_val.astype(int)\n",
    "\n",
    "def YearsAndMonthsToMonths(sentence):\n",
    "    return int(sentence.split()[0]) * 12 + int(sentence.split()[3])\n",
    "# assign proper data type to columns\n",
    "\n",
    "def types_change(train_df):\n",
    "    # Month\n",
    "    from datetime import datetime as dt\n",
    "    train_df['Month'] = train_df['Month'].apply(lambda x: dt.strptime(x, '%B')) \n",
    "    train_df['Month'] = train_df['Month'].dt.month\n",
    "\n",
    "    # Age\n",
    "    #train_df[train_df['Age'].str.contains('_')]['Age']\n",
    "    train_df['Age'] = train_df['Age'].apply(lambda x: x.replace('_','',))\n",
    "    train_df['Age'] = train_df.Age.astype(int)\n",
    "\n",
    "    # Occupation\n",
    "    train_df['Occupation'] = train_df['Occupation'].apply(lambda x: 'Other' if str(x) == '_______' else x)\n",
    "    train_df['Occupation'] = train_df.Occupation.astype(str)\n",
    "\n",
    "    # Annual_Income\n",
    "    train_df['Annual_Income'] = train_df['Annual_Income'].apply(lambda x: x.replace('_','',))\n",
    "    train_df['Annual_Income'] = train_df.Annual_Income.astype(float)\n",
    "\n",
    "    # Num_of_Loan\n",
    "    train_df['Num_of_Loan'] = train_df['Num_of_Loan'].apply(lambda x: x.replace('_','',))\n",
    "    train_df['Num_of_Loan'] = train_df.Num_of_Loan.astype(int)\n",
    "\n",
    "    # Num_of_Delayed_Payment\n",
    "    train_df['Num_of_Delayed_Payment'] = train_df['Num_of_Delayed_Payment'].apply(lambda x: x if x is np.NaN else int(str(x).replace('_','',)))\n",
    "    train_df['Num_of_Delayed_Payment'] = train_df.Num_of_Delayed_Payment.astype(float)\n",
    "\n",
    "    # Credit_Mix\n",
    "    train_df['Credit_Mix'] = train_df['Credit_Mix'].apply(lambda x: np.NaN if str(x) == '_' else x)\n",
    "    numeric_mapping_dict = {'Standard': 0, 'Bad': -1, 'Good': 1}\n",
    "    train_df['Credit_Mix'] = train_df['Credit_Mix'].map(numeric_mapping_dict)\n",
    "\n",
    "    # Changed_Credit_Limit\n",
    "    train_df['Changed_Credit_Limit'] = train_df['Changed_Credit_Limit'].apply(lambda x: np.NaN if str(x) == '_' else x)\n",
    "    train_df['Changed_Credit_Limit'] = train_df.Changed_Credit_Limit.astype(float)\n",
    "\n",
    "    # Outstanding_Debt\n",
    "    train_df['Outstanding_Debt'] = train_df['Outstanding_Debt'].apply(lambda x: x.replace('_','',))\n",
    "    train_df['Outstanding_Debt'] = train_df.Outstanding_Debt.astype(float)\n",
    "\n",
    "    # Credit_History_Age\n",
    "\n",
    "\n",
    "    train_df.Credit_History_Age = train_df.Credit_History_Age.apply(lambda x: x if x is np.NaN else YearsAndMonthsToMonths(x))\n",
    "    train_df['Credit_History_Age'] = train_df.Credit_History_Age.astype(float)\n",
    "\n",
    "    # Amount_invested_monthly\n",
    "    train_df['Amount_invested_monthly'] = train_df['Amount_invested_monthly'].apply(lambda x: x if x is np.NaN else float(str(x).replace('_','',)))\n",
    "    train_df['Amount_invested_monthly'] = train_df.Amount_invested_monthly.astype(float)\n",
    "\n",
    "    # Payment_Behaviour\n",
    "    train_df['Payment_Behaviour'] = train_df['Payment_Behaviour'].apply(lambda x: 'Other' if str(x) == '!@9#%8' else x)\n",
    "    train_df['Payment_Behaviour'] = train_df.Payment_Behaviour.astype(str)\n",
    "\n",
    "    # Monthly_Balance\n",
    "    train_df['Monthly_Balance'] = train_df['Monthly_Balance'].apply(lambda x: float(str(x).replace('_','',)))\n",
    "    train_df['Monthly_Balance'] = train_df.Monthly_Balance.astype(float)\n",
    "\n",
    "    # Payment_of_Min_Amount\n",
    "    numeric_mapping_dict = {'NM': 0, 'No': -1, 'Yes': 1}\n",
    "    train_df['Payment_of_Min_Amount'] = train_df['Payment_of_Min_Amount'].map(numeric_mapping_dict)\n",
    "    train_df['Monthly_Balance'] = train_df.Monthly_Balance.astype(float)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Occupation - change to numeric\n",
    "    train_df.Occupation = le.fit_transform(train_df.Occupation)\n",
    "    train_df['Occupation'] = train_df.Occupation.astype(int)\n",
    "\n",
    "    # Payment_Behaviour - change to numeric\n",
    "\n",
    "    train_df.Payment_Behaviour = le.fit_transform(train_df.Payment_Behaviour)\n",
    "    train_df['Payment_Behaviour'] = train_df.Payment_Behaviour.astype(int)\n",
    "\n",
    "    \n",
    "\n",
    "types_change(train_df)\n",
    "types_change(test_df)\n",
    "types_change(validation_df)\n",
    "\n",
    "\n",
    "\n",
    "# train_df.info()\n",
    "# test_df.info()\n",
    "# validation_df.info()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Left to be done: \n",
    "# Type_of_Loan (modify categories inside) - propably split to multiple columns\n",
    "#DONE Change to int Num_of_Delayed_Payment, Num_Credit_Inquiries\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "# # assign proper data type to columns\n",
    "# train_df = copy.deepcopy(og_train_df)\n",
    "# # Month\n",
    "# from datetime import datetime as dt\n",
    "# train_df['Month'] = train_df['Month'].apply(lambda x: dt.strptime(x, '%B')) \n",
    "# train_df['Month'] = train_df['Month'].dt.month\n",
    "\n",
    "# # Age\n",
    "# #train_df[train_df['Age'].str.contains('_')]['Age']\n",
    "# train_df['Age'] = train_df['Age'].apply(lambda x: x.replace('_','',))\n",
    "# train_df['Age'] = train_df.Age.astype(int)\n",
    "\n",
    "# # Occupation\n",
    "# train_df['Occupation'] = train_df['Occupation'].apply(lambda x: 'Other' if str(x) == '_______' else x)\n",
    "# train_df['Occupation'] = train_df.Occupation.astype(str)\n",
    "\n",
    "# # Annual_Income\n",
    "# train_df['Annual_Income'] = train_df['Annual_Income'].apply(lambda x: x.replace('_','',))\n",
    "# train_df['Annual_Income'] = train_df.Annual_Income.astype(float)\n",
    "\n",
    "# # Num_of_Loan\n",
    "# train_df['Num_of_Loan'] = train_df['Num_of_Loan'].apply(lambda x: x.replace('_','',))\n",
    "# train_df['Num_of_Loan'] = train_df.Num_of_Loan.astype(int)\n",
    "\n",
    "# # Num_of_Delayed_Payment\n",
    "# train_df['Num_of_Delayed_Payment'] = train_df['Num_of_Delayed_Payment'].apply(lambda x: x if x is np.NaN else int(str(x).replace('_','',)))\n",
    "# train_df['Num_of_Delayed_Payment'] = train_df.Num_of_Delayed_Payment.astype(float)\n",
    "\n",
    "# # Credit_Mix\n",
    "# train_df['Credit_Mix'] = train_df['Credit_Mix'].apply(lambda x: np.NaN if str(x) == '_' else x)\n",
    "# numeric_mapping_dict = {'Standard': 0, 'Bad': -1, 'Good': 1}\n",
    "# train_df['Credit_Mix'] = train_df['Credit_Mix'].map(numeric_mapping_dict)\n",
    "\n",
    "# # Changed_Credit_Limit\n",
    "# train_df['Changed_Credit_Limit'] = train_df['Changed_Credit_Limit'].apply(lambda x: np.NaN if str(x) == '_' else x)\n",
    "# train_df['Changed_Credit_Limit'] = train_df.Changed_Credit_Limit.astype(float)\n",
    "\n",
    "# # Outstanding_Debt\n",
    "# train_df['Outstanding_Debt'] = train_df['Outstanding_Debt'].apply(lambda x: x.replace('_','',))\n",
    "# train_df['Outstanding_Debt'] = train_df.Outstanding_Debt.astype(float)\n",
    "\n",
    "# # Credit_History_Age\n",
    "# def YearsAndMonthsToMonths(sentence):\n",
    "#     return int(sentence.split()[0]) * 12 + int(sentence.split()[3])\n",
    "\n",
    "# train_df.Credit_History_Age = train_df.Credit_History_Age.apply(lambda x: x if x is np.NaN else YearsAndMonthsToMonths(x))\n",
    "# train_df['Credit_History_Age'] = train_df.Credit_History_Age.astype(float)\n",
    "\n",
    "# # Amount_invested_monthly\n",
    "# train_df['Amount_invested_monthly'] = train_df['Amount_invested_monthly'].apply(lambda x: x if x is np.NaN else float(str(x).replace('_','',)))\n",
    "# train_df['Amount_invested_monthly'] = train_df.Amount_invested_monthly.astype(float)\n",
    "\n",
    "# # Payment_Behaviour\n",
    "# train_df['Payment_Behaviour'] = train_df['Payment_Behaviour'].apply(lambda x: 'Other' if str(x) == '!@9#%8' else x)\n",
    "# train_df['Payment_Behaviour'] = train_df.Payment_Behaviour.astype(str)\n",
    "\n",
    "# # Monthly_Balance\n",
    "# train_df['Monthly_Balance'] = train_df['Monthly_Balance'].apply(lambda x: float(str(x).replace('_','',)))\n",
    "# train_df['Monthly_Balance'] = train_df.Monthly_Balance.astype(float)\n",
    "\n",
    "# # Payment_of_Min_Amount\n",
    "# numeric_mapping_dict = {'NM': 0, 'No': -1, 'Yes': 1}\n",
    "# train_df['Payment_of_Min_Amount'] = train_df['Payment_of_Min_Amount'].map(numeric_mapping_dict)\n",
    "# train_df['Monthly_Balance'] = train_df.Monthly_Balance.astype(float)\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "\n",
    "# # Occupation - change to numeric\n",
    "# train_df.Occupation = le.fit_transform(train_df.Occupation)\n",
    "# train_df['Occupation'] = train_df.Occupation.astype(int)\n",
    "\n",
    "# # Payment_Behaviour - change to numeric\n",
    "\n",
    "# train_df.Payment_Behaviour = le.fit_transform(train_df.Payment_Behaviour)\n",
    "# train_df['Payment_Behaviour'] = train_df.Payment_Behaviour.astype(int)\n",
    "\n",
    "# # Credit_Score - change to numeric\n",
    "# numeric_mapping_dict = {'Standard': 0, 'Poor': -1, 'Good': 1}\n",
    "# train_df['Credit_Score'] = train_df['Credit_Score'].map(numeric_mapping_dict)\n",
    "# train_df['Credit_Score'] = train_df.Credit_Score.astype(int)\n",
    "# train_df.info()\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "#for col in train_df.columns:\n",
    "#   print(\"Column name: \" + col + \", dtype: \" + str(train_df[col].dtype))\n",
    "#     print(train_df[col].value_counts(dropna = False))\n",
    "#     print(\"\\n\")\n",
    "\n",
    "\n",
    "# Dealing with NaNs, outliers and unrelevant columns\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "# TODO\n",
    "#DONE Nan certain columns: ID, Customer_ID, Name, SSN (social security number of a person)\n",
    "\n",
    "#DONE fulfill NaNs, aviable options: \n",
    "## DROP: drop rows, drop columns\n",
    "## IMPUTE: mean (not skewed) or median (not sensitive to outliers), linear regression\n",
    "## FLAG: changing to 'Other' value\n",
    "\n",
    "#DONE outliers, check: Annual_Income, Num_Bank_Accounts, Num_Credit_Card, Interest_Rate, Num_of_Loan, Num_of_Delayed_Payment, Num_Credit_Inquiries, Monthly_Balance\n",
    "#DONE see distribution of features, for sure: hist and boxplot\n",
    "#DONE Num_of_Loan has negative values (check other nums)\n",
    "#DONE check if some columns could be ints instead of floats\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "\n",
    "# #### Serving outliers and fulfilling NaNs\n",
    "\n",
    "# ### Monthly_Inhand_Salary\n",
    "\n",
    "# #  (code to check distribution of the data)\n",
    "# #  train_df.Monthly_Inhand_Salary.isna().sum()/train_df.shape[0]*100\n",
    "# #  train_df.Monthly_Inhand_Salary.value_counts()\n",
    "# #  train_df.Monthly_Inhand_Salary.describe()\n",
    "# #  sns.histplot(train_df.Monthly_Inhand_Salary)\n",
    "# #  sns.boxplot(train_df.Monthly_Inhand_Salary)\n",
    "# #  plt.show()\n",
    "\n",
    "\n",
    "# ## DECISION: outliers match distribution of the feature, imputing median\n",
    "\n",
    "# m = train_df.Monthly_Inhand_Salary.median()\n",
    "# train_df['Monthly_Inhand_Salary'] = train_df['Monthly_Inhand_Salary'].apply(lambda x: m if np.isnan(x) else x)\n",
    "\n",
    "\n",
    "\n",
    "# ### Num_of_Delayed_Payment\n",
    "\n",
    "# # (code to check distribution of the data)\n",
    "# # train_df.Num_of_Delayed_Payment.isna().sum()/train_df.shape[0]*100\n",
    "# # train_df.Num_of_Delayed_Payment.value_counts()\n",
    "# # train_df.Num_of_Delayed_Payment.describe()\n",
    "# # sns.histplot(train_df.Num_of_Delayed_Payment, bins=50)\n",
    "# # sns.boxplot(train_df.Num_of_Delayed_Payment)\n",
    "# # sns.displot(train_df.Num_of_Delayed_Payment, kind=\"ecdf\")\n",
    "# # plt.show()\n",
    "\n",
    "# # checking outliers \n",
    "# # (code to check distribution of the data)\n",
    "# # train_df.sort_values(by=\"Num_of_Delayed_Payment\", ascending=False)[['Num_of_Delayed_Payment', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_of_Loan', 'Amount_invested_monthly', 'Monthly_Balance']].head(20)\n",
    "# # centile99Annual_Income = train_df.Annual_Income.quantile(0.99)\n",
    "# # centile99Monthly_Balance = train_df.Monthly_Balance.quantile(0.99)\n",
    "# # train_df[(train_df['Annual_Income'] > centile95Annual_Income) & (train_df['Monthly_Balance'] > centile95Monthly_Balance)].sort_values(by=\"Num_of_Delayed_Payment\", ascending=False)[[\n",
    "# #     'Num_of_Delayed_Payment', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_of_Loan', 'Amount_invested_monthly', 'Monthly_Balance']].head(20)\n",
    "# # they are clearly outliers as they do not correspond to other features, have to be cut\n",
    "\n",
    "\n",
    "# ## DECISION: Num_of_Delayed_Payment has to be positive, outliers have to be cut, imputing median\n",
    "\n",
    "# q = train_df.Num_of_Delayed_Payment.quantile(0.95)\n",
    "# train_df['Num_of_Delayed_Payment'] = train_df['Num_of_Delayed_Payment'].apply(lambda x: q if x>q else x)\n",
    "\n",
    "# m = train_df.Num_of_Delayed_Payment.median()\n",
    "# train_df['Num_of_Delayed_Payment'] = train_df['Num_of_Delayed_Payment'].apply(lambda x: m if np.isnan(x) or x<0 else x)\n",
    "\n",
    "\n",
    "\n",
    "# ### Num_Credit_Inquiries         \n",
    "\n",
    "# # (code to check distribution of the data)\n",
    "# # train_df.Num_Credit_Inquiries.describe()\n",
    "# # train_df.Num_Credit_Inquiries.value_counts()\n",
    "# # sns.histplot(train_df.Num_Credit_Inquiries, bins=50)\n",
    "# # sns.boxplot(train_df.Num_Credit_Inquiries)\n",
    "# # sns.displot(train_df.Num_Credit_Inquiries, kind=\"ecdf\")\n",
    "# # plt.show()\n",
    "\n",
    "# # checking outliers \n",
    "# # (code to check distribution of the data)\n",
    "# # train_df.sort_values(by=\"Num_Credit_Inquiries\", ascending=False)[['Num_Credit_Inquiries', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_of_Loan', 'Amount_invested_monthly', 'Monthly_Balance']].head(20)\n",
    "# # centile99Annual_Income = train_df.Annual_Income.quantile(0.99)\n",
    "# # centile99Monthly_Balance = train_df.Monthly_Balance.quantile(0.99)\n",
    "# # train_df[(train_df['Annual_Income'] > centile95Annual_Income) & (train_df['Monthly_Balance'] > centile95Monthly_Balance)].sort_values(by=\"Num_Credit_Inquiries\", ascending=False)[[\n",
    "# #    'Num_Credit_Inquiries', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_of_Loan', 'Amount_invested_monthly', 'Monthly_Balance']].head(20)\n",
    "# # they are clearly outliers as they do not correspond to other features, have to be cut\n",
    "\n",
    "\n",
    "# ## DECISION: outliers need to be cut, imputing median\n",
    "\n",
    "# q = train_df.Num_Credit_Inquiries.quantile(0.98)\n",
    "# train_df['Num_Credit_Inquiries'] = train_df['Num_Credit_Inquiries'].apply(lambda x: q if x>q else x)\n",
    "\n",
    "# m = train_df.Num_Credit_Inquiries.median()\n",
    "# train_df['Num_Credit_Inquiries'] = train_df['Num_Credit_Inquiries'].apply(lambda x: m if np.isnan(x) else x)\n",
    "\n",
    "\n",
    "\n",
    "# ### Credit_History_Age\n",
    "\n",
    "# # (code to check distribution of the data)\n",
    "# # train_df.Credit_History_Age.describe()\n",
    "# # sns.histplot(train_df.Credit_History_Age, bins=50)\n",
    "# # sns.boxplot(train_df.Credit_History_Age)\n",
    "# # sns.displot(train_df.Credit_History_Age, kind=\"ecdf\")\n",
    "# # plt.show()\n",
    "\n",
    "\n",
    "# ## DECISION: there are no outliers, imputing median\n",
    "\n",
    "# m = train_df.Credit_History_Age.median()\n",
    "# train_df['Credit_History_Age'] = train_df['Credit_History_Age'].apply(lambda x: m if np.isnan(x) else x)\n",
    "\n",
    "\n",
    "\n",
    "# ### Credit_Mix\n",
    "\n",
    "# # (code to check distribution of the data)\n",
    "# # train_df.Credit_Mix.describe()\n",
    "# # train_df.Credit_Mix.value_counts()\n",
    "# # sns.histplot(train_df.Credit_Mix)\n",
    "# # sns.displot(train_df.Credit_Mix, kind=\"ecdf\")\n",
    "# # plt.show()\n",
    "\n",
    "# ## DECISION no outliers, imputing median\n",
    "\n",
    "# m = train_df.Credit_Mix.median()\n",
    "# train_df['Credit_Mix'] = train_df['Credit_Mix'].apply(lambda x: m if np.isnan(x) else x) \n",
    "# # imputing with median seems reasonable, it is 0 value, which is neutral to 2 other, unique values in the column: -1 and 1\n",
    "\n",
    "\n",
    "\n",
    "# ### Amount_invested_monthly \n",
    "  \n",
    "# # (code to check distribution of the data)\n",
    "# # train_df.Amount_invested_monthly.describe()\n",
    "# # sns.histplot(train_df.Amount_invested_monthly, bins=50)\n",
    "# # sns.boxplot(train_df.Amount_invested_monthly)\n",
    "# # sns.displot(train_df.Amount_invested_monthly, kind=\"ecdf\")\n",
    "# # plt.show()\n",
    "\n",
    "# # checking outliers \n",
    "# # centile95Annual_Income = train_df.Annual_Income.quantile(0.95)\n",
    "# # centile95Monthly_Balance = train_df.Monthly_Balance.quantile(0.95)\n",
    "# # train_df.sort_values(by=\"Amount_invested_monthly\", ascending=False)[[\n",
    "# #    'Amount_invested_monthly', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_of_Loan', 'Amount_invested_monthly', 'Monthly_Balance']].head(20)\n",
    "\n",
    "\n",
    "# ## DECISION: outliers need to be cut, imputing median\n",
    "\n",
    "# iqr = train_df.Amount_invested_monthly.quantile(0.75) - train_df.Amount_invested_monthly.quantile(0.25)\n",
    "# top_border = train_df.Amount_invested_monthly.quantile(0.75) + 1.5*iqr\n",
    "# train_df['Amount_invested_monthly'] = train_df['Amount_invested_monthly'].apply(lambda x: top_border if x>top_border else x)\n",
    "# # TODO: what about values inbeetwen 125-135 (strange look on histplot)\n",
    "\n",
    "\n",
    "\n",
    "# m = train_df.Amount_invested_monthly.median()\n",
    "# train_df['Amount_invested_monthly'] = train_df['Amount_invested_monthly'].apply(lambda x: m if np.isnan(x) else x) \n",
    "\n",
    "\n",
    "\n",
    "# ### Monthly_Balance \n",
    "\n",
    "# # (code to check distribution of the data)\n",
    "# # train_df.Monthly_Balance.describe()\n",
    "# # sns.histplot(train_df.Monthly_Balance, bins=50)\n",
    "# # sns.boxplot(train_df.Monthly_Balance)\n",
    "# # sns.displot(train_df.Monthly_Balance, kind=\"ecdf\")\n",
    "# # plt.show()\n",
    "\n",
    "\n",
    "# ## DECISION: outloutliers need to be cut, imputing median\n",
    "\n",
    "# train_df['Monthly_Balance'] = train_df['Monthly_Balance'].apply(lambda x: 7.759665e-03 if x<-3.333333e+26 else x)\n",
    "# train_df['Monthly_Balance'].describe()\n",
    "\n",
    "# m = train_df.Monthly_Balance.median()\n",
    "# train_df['Monthly_Balance'] = train_df['Monthly_Balance'].apply(lambda x: m if np.isnan(x) else x) \n",
    "\n",
    "\n",
    "\n",
    "# ### Changed_Credit_Limit  \n",
    " \n",
    "# # (code to check distribution of the data)\n",
    "# # train_df.Changed_Credit_Limit.describe()\n",
    "# # sns.histplot(train_df.Changed_Credit_Limit, bins=50)\n",
    "# # sns.boxplot(train_df.Changed_Credit_Limit)\n",
    "# # sns.displot(train_df.Changed_Credit_Limit, kind=\"ecdf\")\n",
    "# # plt.show()\n",
    "\n",
    "\n",
    "# ## DECISION: outliers match distribution of the feature, imputing median \n",
    "\n",
    "# m = train_df.Changed_Credit_Limit.median()\n",
    "# train_df['Changed_Credit_Limit'] = train_df['Changed_Credit_Limit'].apply(lambda x: m if np.isnan(x) else x)\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "def outliers_change(train_df):\n",
    "    m = train_df.Monthly_Inhand_Salary.median()\n",
    "    train_df['Monthly_Inhand_Salary'] = train_df['Monthly_Inhand_Salary'].apply(lambda x: m if np.isnan(x) else x)\n",
    "\n",
    "\n",
    "    q = train_df.Num_of_Delayed_Payment.quantile(0.95)\n",
    "    train_df['Num_of_Delayed_Payment'] = train_df['Num_of_Delayed_Payment'].apply(lambda x: q if x>q else x)\n",
    "\n",
    "    m = train_df.Num_of_Delayed_Payment.median()\n",
    "    train_df['Num_of_Delayed_Payment'] = train_df['Num_of_Delayed_Payment'].apply(lambda x: m if np.isnan(x) or x<0 else x)\n",
    "\n",
    "\n",
    "    q = train_df.Num_Credit_Inquiries.quantile(0.98)\n",
    "    train_df['Num_Credit_Inquiries'] = train_df['Num_Credit_Inquiries'].apply(lambda x: q if x>q else x)\n",
    "\n",
    "    m = train_df.Num_Credit_Inquiries.median()\n",
    "    train_df['Num_Credit_Inquiries'] = train_df['Num_Credit_Inquiries'].apply(lambda x: m if np.isnan(x) else x)\n",
    "\n",
    "    m = train_df.Credit_History_Age.median()\n",
    "    train_df['Credit_History_Age'] = train_df['Credit_History_Age'].apply(lambda x: m if np.isnan(x) else x)\n",
    "\n",
    "    m = train_df.Credit_Mix.median()\n",
    "    train_df['Credit_Mix'] = train_df['Credit_Mix'].apply(lambda x: m if np.isnan(x) else x) \n",
    "\n",
    "\n",
    "    iqr = train_df.Amount_invested_monthly.quantile(0.75) - train_df.Amount_invested_monthly.quantile(0.25)\n",
    "    top_border = train_df.Amount_invested_monthly.quantile(0.75) + 1.5*iqr\n",
    "    train_df['Amount_invested_monthly'] = train_df['Amount_invested_monthly'].apply(lambda x: top_border if x>top_border else x)\n",
    "\n",
    "\n",
    "    m = train_df.Amount_invested_monthly.median()\n",
    "    train_df['Amount_invested_monthly'] = train_df['Amount_invested_monthly'].apply(lambda x: m if np.isnan(x) else x) \n",
    "\n",
    "\n",
    "\n",
    "    train_df['Monthly_Balance'] = train_df['Monthly_Balance'].apply(lambda x: 7.759665e-03 if x<-3.333333e+26 else x)\n",
    "    train_df['Monthly_Balance'].describe()\n",
    "\n",
    "    m = train_df.Monthly_Balance.median()\n",
    "    train_df['Monthly_Balance'] = train_df['Monthly_Balance'].apply(lambda x: m if np.isnan(x) else x)\n",
    "\n",
    "    m = train_df.Changed_Credit_Limit.median()\n",
    "    train_df['Changed_Credit_Limit'] = train_df['Changed_Credit_Limit'].apply(lambda x: m if np.isnan(x) else x)\n",
    "\n",
    "outliers_change(train_df)\n",
    "outliers_change(test_df)\n",
    "outliers_change(validation_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "# # check for outliers and uncorrectness in remaining columns\n",
    "\n",
    "# # To all is used at least:\n",
    "# # train_df.X.describe() / train_df.X.value_counts()\n",
    "# # sns.histplot(train_df.X, bins=50)\n",
    "# # sns.boxplot(train_df.X)\n",
    "# # sns.displot(train_df.X, kind=\"ecdf\")\n",
    "# # plt.show()\n",
    "\n",
    "# ### Age\n",
    "\n",
    "# m = train_df.Age.median()\n",
    "# train_df['Age'] = train_df['Age'].apply(lambda x: m if x < 15 or x > 110 else x)\n",
    "\n",
    "# # outliery\n",
    "# q = train_df.Age.quantile(0.95)\n",
    "# train_df['Age'] = train_df['Age'].apply(lambda x: q if x>q else x)\n",
    "\n",
    "# ### Annual_Income\n",
    "\n",
    "\n",
    "# #train_df.Annual_Income.describe()\n",
    "# #train_df.sort_values(['Annual_Income'], ascending=False).head(20)\n",
    "# #sns.histplot(np.log1p(train_df.Annual_Income), bins=20)\n",
    "# #plt.show()\n",
    "# #Conclusion: there top_border are outliers\n",
    "\n",
    "\n",
    "# iqr = train_df.Annual_Income.quantile(0.75) - train_df.Annual_Income.quantile(0.25)\n",
    "# top_border = train_df.Annual_Income.quantile(0.75) + 1.5*iqr\n",
    "# train_df['Annual_Income'] = train_df['Annual_Income'].apply(lambda x: top_border if x>top_border else x)\n",
    "\n",
    "# # Num_Bank_Accounts\n",
    "\n",
    "# # train_df.Num_Bank_Accounts.describe()\n",
    "# # train_df.sort_values(['Num_Bank_Accounts'], ascending=False)[['Num_Bank_Accounts', 'Num_Credit_Card']].head(20) # shows that it is an anomaly\n",
    "# # sns.histplot(np.log1p(train_df.Num_Bank_Accounts), bins=20)\n",
    "# # sns.displot(train_df.Num_Bank_Accounts, kind=\"ecdf\")\n",
    "# # plt.show()\n",
    "# # Conclusion: getting rid of negative values and cutting outliers\n",
    "\n",
    "# m = train_df.Num_Bank_Accounts.median()\n",
    "# train_df['Num_Bank_Accounts'] = train_df['Num_Bank_Accounts'].apply(lambda x: m if x < 0 else x)\n",
    "\n",
    "# q = train_df.Num_Bank_Accounts.quantile(0.95)\n",
    "# train_df['Num_Bank_Accounts'] = train_df['Num_Bank_Accounts'].apply(lambda x: q if x>q else x)\n",
    "\n",
    "\n",
    "# # Num_Credit_Card\n",
    "\n",
    "# # train_df.Num_Credit_Card.describe()\n",
    "# # train_df.sort_values(['Num_Credit_Card'], ascending=False)[['Num_Bank_Accounts', 'Num_Credit_Card']].head(20) # shows that it is an anomaly\n",
    "# # sns.displot(train_df.Num_Credit_Card, kind=\"ecdf\")\n",
    "# # plt.show()\n",
    "# # Conclusion: cutting outliers\n",
    "\n",
    "# q = train_df.Num_Credit_Card.quantile(0.95)\n",
    "# train_df['Num_Credit_Card'] = train_df['Num_Credit_Card'].apply(lambda x: q if x>q else x)\n",
    "\n",
    "# # Interest_Rate\n",
    "\n",
    "# # train_df.Interest_Rate.describe()\n",
    "# # train_df.sort_values(['Interest_Rate'], ascending=False).head(20)\n",
    "# # sns.displot(train_df.Interest_Rate, kind=\"ecdf\")\n",
    "# # sns.histplot(np.log1p(train_df.Interest_Rate), bins=20)\n",
    "# # plt.show()\n",
    "# # Conclusion: cutting outliers\n",
    "\n",
    "# q = train_df.Interest_Rate.quantile(0.95)\n",
    "# train_df['Interest_Rate'] = train_df['Interest_Rate'].apply(lambda x: q if x>q else x)\n",
    "\n",
    "# # Num_of_Loan\n",
    "\n",
    "# # train_df.Num_of_Loan.describe()\n",
    "# # train_df.sort_values(['Num_of_Loan'], ascending=False).head(20)\n",
    "# # sns.displot(train_df.Num_of_Loan, kind=\"ecdf\")\n",
    "# # sns.histplot(np.log1p(train_df.Num_of_Loan), bins=20)\n",
    "# # plt.show()\n",
    "# # Conclusion: cutting outliers\n",
    "\n",
    "# m = train_df.Num_of_Loan.median()\n",
    "# train_df['Num_of_Loan'] = train_df['Num_of_Loan'].apply(lambda x: m if x < 0 else x)\n",
    "\n",
    "# q = train_df.Num_of_Loan.quantile(0.95)\n",
    "# train_df['Num_of_Loan'] = train_df['Num_of_Loan'].apply(lambda x: q if x>q else x)\n",
    "\n",
    "# # Delay_from_due_date\n",
    "\n",
    "# # train_df.Delay_from_due_date.describe()\n",
    "# # sns.displot(train_df.Delay_from_due_date, kind=\"ecdf\")\n",
    "# # sns.histplot(np.log1p(train_df.Delay_from_due_date), bins=20)\n",
    "# # plt.show()\n",
    "# # Conclusion: imputing median for negative values\n",
    "\n",
    "# m = train_df.Num_of_Loan.median()\n",
    "# train_df['Delay_from_due_date'] = train_df['Delay_from_due_date'].apply(lambda x: m if x < 0 else x)\n",
    "\n",
    "# # Outstanding_Debt\n",
    "\n",
    "# # train_df.Outstanding_Debt.describe()\n",
    "# # sns.displot(train_df.Outstanding_Debt, kind=\"ecdf\")\n",
    "# # sns.histplot(np.log1p(train_df.Outstanding_Debt), bins=20)\n",
    "# # plt.show()\n",
    "# # Conclusion: everyting fine\n",
    "\n",
    "# # Credit_Utilization_Ratio\n",
    "\n",
    "# # train_df.Credit_Utilization_Ratio.describe()\n",
    "# # sns.displot(train_df.Credit_Utilization_Ratio, kind=\"ecdf\")\n",
    "# # sns.histplot(np.log1p(train_df.Credit_Utilization_Ratio), bins=20)\n",
    "# # plt.show()\n",
    "# # Conclusion: everything fine\n",
    "\n",
    "# # Total_EMI_per_month\n",
    "\n",
    "# # train_df.Total_EMI_per_month.describe()\n",
    "# # train_df.sort_values(['Total_EMI_per_month'], ascending=False).head(20)\n",
    "# # sns.displot(train_df.Total_EMI_per_month, kind=\"ecdf\")\n",
    "# # sns.histplot(np.log1p(train_df.Total_EMI_per_month), bins=20)\n",
    "# # plt.show()\n",
    "# # Conclusion: cutting outliers\n",
    "\n",
    "# q = train_df.Total_EMI_per_month.quantile(0.95)\n",
    "# train_df['Total_EMI_per_month'] = train_df['Total_EMI_per_month'].apply(lambda x: q if x>q else x)\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "def outliers_change_2(train_df):\n",
    "    m = train_df.Age.median()\n",
    "    train_df['Age'] = train_df['Age'].apply(lambda x: m if x < 15 or x > 110 else x)\n",
    "\n",
    "\n",
    "    q = train_df.Age.quantile(0.95)\n",
    "    train_df['Age'] = train_df['Age'].apply(lambda x: q if x>q else x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    iqr = train_df.Annual_Income.quantile(0.75) - train_df.Annual_Income.quantile(0.25)\n",
    "    top_border = train_df.Annual_Income.quantile(0.75) + 1.5*iqr\n",
    "    train_df['Annual_Income'] = train_df['Annual_Income'].apply(lambda x: top_border if x>top_border else x)\n",
    "\n",
    "\n",
    "\n",
    "    m = train_df.Num_Bank_Accounts.median()\n",
    "    train_df['Num_Bank_Accounts'] = train_df['Num_Bank_Accounts'].apply(lambda x: m if x < 0 else x)\n",
    "\n",
    "    q = train_df.Num_Bank_Accounts.quantile(0.95)\n",
    "    train_df['Num_Bank_Accounts'] = train_df['Num_Bank_Accounts'].apply(lambda x: q if x>q else x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    q = train_df.Num_Credit_Card.quantile(0.95)\n",
    "    train_df['Num_Credit_Card'] = train_df['Num_Credit_Card'].apply(lambda x: q if x>q else x)\n",
    "\n",
    "\n",
    "\n",
    "    q = train_df.Interest_Rate.quantile(0.95)\n",
    "    train_df['Interest_Rate'] = train_df['Interest_Rate'].apply(lambda x: q if x>q else x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    m = train_df.Num_of_Loan.median()\n",
    "    train_df['Num_of_Loan'] = train_df['Num_of_Loan'].apply(lambda x: m if x < 0 else x)\n",
    "\n",
    "    q = train_df.Num_of_Loan.quantile(0.95)\n",
    "    train_df['Num_of_Loan'] = train_df['Num_of_Loan'].apply(lambda x: q if x>q else x)\n",
    "\n",
    "\n",
    "\n",
    "    m = train_df.Num_of_Loan.median()\n",
    "    train_df['Delay_from_due_date'] = train_df['Delay_from_due_date'].apply(lambda x: m if x < 0 else x)\n",
    "\n",
    "\n",
    "\n",
    "    q = train_df.Total_EMI_per_month.quantile(0.95)\n",
    "    train_df['Total_EMI_per_month'] = train_df['Total_EMI_per_month'].apply(lambda x: q if x>q else x)\n",
    "\n",
    "outliers_change_2(train_df)\n",
    "outliers_change_2(test_df)\n",
    "outliers_change_2(validation_df)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "#train_df.Type_of_Loan.info()\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "# Changing columns that have int values, but are in float categories (couldn't be done earlier, because there weer NaNs)\n",
    "def changing_to_int(train_df):\n",
    "    train_df['Age'] = train_df.Age.astype(int)\n",
    "    train_df['Num_Bank_Accounts'] = train_df.Num_Bank_Accounts.astype(int)\n",
    "    train_df['Num_Credit_Card'] = train_df.Num_Credit_Card.astype(int)\n",
    "    train_df['Interest_Rate'] = train_df.Interest_Rate.astype(int)\n",
    "    train_df['Num_of_Loan'] = train_df.Num_of_Loan.astype(int)\n",
    "    train_df['Delay_from_due_date'] = train_df.Delay_from_due_date.astype(int)\n",
    "    train_df['Credit_History_Age'] = train_df.Credit_History_Age.astype(int)\n",
    "    train_df['Num_of_Delayed_Payment'] = train_df.Num_of_Delayed_Payment.astype(int)\n",
    "    train_df['Num_Credit_Inquiries'] = train_df.Num_Credit_Inquiries.astype(int)\n",
    "    train_df['Credit_Mix'] = train_df.Credit_Mix.astype(int)\n",
    "    train_df['Credit_History_Age'] = train_df.Credit_History_Age.astype(int)\n",
    "    \n",
    "changing_to_int(train_df)\n",
    "changing_to_int(test_df)\n",
    "changing_to_int(validation_df)\n",
    "\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "# train_df['Type_of_Loan'].isnull().sum()\n",
    "# train_df['Type_of_Loan'] = train_df['Type_of_Loan'].fillna('Other')\n",
    "# train_df['Type_of_Loan'].isnull().sum()\n",
    "# # Deleting NA\n",
    "# train_df['Type_of_Loan'] = train_df['Type_of_Loan'].fillna('Other')\n",
    "\n",
    "# # Adding columns for all types of loans\n",
    "# types_of_loans_dict = {}\n",
    "# for raw_value in train_df['Type_of_Loan'].value_counts().index[:]:\n",
    "    \n",
    "#     raw_value = raw_value.replace('and','')\n",
    "#     list_of_loan_types = raw_value.split(',')\n",
    "\n",
    "#     for j in range(len(list_of_loan_types)):\n",
    "#         list_of_loan_types[j] = list_of_loan_types[j].replace(' ','')\n",
    "\n",
    "#     for k in list_of_loan_types:\n",
    "#         if k not in types_of_loans_dict.values():\n",
    "#             types_of_loans_dict[len(types_of_loans_dict)] = k\n",
    "\n",
    "# zeros = [0] * train_df.shape[0]\n",
    "# for i in types_of_loans_dict.values():\n",
    "#     train_df[i] = zeros\n",
    "\n",
    "# display(train_df)\n",
    "# train_df.shape\n",
    "# train_df.Type_of_Loan[0]\n",
    "\n",
    "\n",
    "# for index, value in train_df['Type_of_Loan'].iteritems():\n",
    "#     print(f\"Index: {index}, Value: {value}\")\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "def changing_loans(train_df):\n",
    "    # Deleting NA\n",
    "    train_df['Type_of_Loan'] = train_df['Type_of_Loan'].fillna('Other')\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    # Adding columns for all types of loans\n",
    "    types_of_loans_dict = {}\n",
    "    for raw_value in train_df['Type_of_Loan'].value_counts().index[:]:\n",
    "\n",
    "        raw_value = raw_value.replace('and','')\n",
    "        list_of_loan_types = raw_value.split(',')\n",
    "\n",
    "        for j in range(len(list_of_loan_types)):\n",
    "            list_of_loan_types[j] = list_of_loan_types[j].replace(' ','')\n",
    "\n",
    "        for k in list_of_loan_types:\n",
    "            if k not in types_of_loans_dict.values():\n",
    "                types_of_loans_dict[len(types_of_loans_dict)] = k\n",
    "\n",
    "    zeros = [0] * train_df.shape[0]\n",
    "    for i in types_of_loans_dict.values():\n",
    "        train_df[i] = zeros\n",
    "\n",
    "    # Transforming type_of_loan into seperate columns\n",
    "    for i in range(len(train_df['Type_of_Loan'])):\n",
    "        raw_value = train_df['Type_of_Loan'][i]\n",
    "        raw_value = raw_value.replace('and','')\n",
    "        list_of_loan_types = raw_value.split(',')\n",
    "\n",
    "        for j in range(len(list_of_loan_types)):\n",
    "            list_of_loan_types[j] = list_of_loan_types[j].replace(' ','')\n",
    "\n",
    "        for k in list_of_loan_types:\n",
    "           train_df[k][i] = 1\n",
    "\n",
    "    train_df = train_df.drop('Type_of_Loan', axis=1)\n",
    "\n",
    "changing_loans(train_df)\n",
    "changing_loans(test_df)\n",
    "changing_loans(validation_df)\n",
    "train_df\n",
    "test_df\n",
    "validation_df\n",
    "\n",
    "train_df = train_df.drop('Credit_Score', axis=1)\n",
    "train_df = train_df.drop('Type_of_Loan', axis=1)\n",
    "test_df = test_df.drop('Type_of_Loan', axis=1)\n",
    "validation_df = validation_df.drop('Type_of_Loan', axis=1)\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "# dropping irrelevant columns\n",
    "train_df = train_df.drop(['Name', 'ID', 'Customer_ID', 'SSN'], axis=1)\n",
    "test_df = test_df.drop(['Name', 'ID', 'Customer_ID', 'SSN'], axis=1)\n",
    "validation_df = validation_df.drop(['Name', 'ID', 'Customer_ID', 'SSN'], axis=1)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "\n",
    "# DONE\n",
    "#  do normalizacji:\n",
    "#  Annual_Income\n",
    "#  Monthly_Inhand_Salary\n",
    "#  Outstanding_Debt\n",
    "#  Credit_History_Age\n",
    "#  Total_EMI_per_month\n",
    "#  Amount_invested_monthly\t\n",
    "#  Monthly_Balance\n",
    "\n",
    "# QUESTION\n",
    "# should we normalize data from days columns?\n",
    "def normalization_df(train_df):\n",
    "    cols_to_scale = ['Annual_Income',\n",
    "    'Monthly_Inhand_Salary',\n",
    "    'Outstanding_Debt',\n",
    "    'Credit_History_Age',\n",
    "    'Total_EMI_per_month',\n",
    "    'Amount_invested_monthly',\t\n",
    "    'Monthly_Balance']\n",
    "\n",
    "    # counting of min and max value in each column\n",
    "    min_vals = train_df[cols_to_scale].min()\n",
    "    max_vals = train_df[cols_to_scale].max()\n",
    "\n",
    "    # scaling values to be in (0,1)\n",
    "    scaled_data = (train_df[cols_to_scale] - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "    # adding scaled data to dataframe\n",
    "    train_df[cols_to_scale] = scaled_data\n",
    "normalization_df(train_df)\n",
    "normalization_df(test_df)\n",
    "normalization_df(validation_df)\n",
    "\n",
    "# test_df.describe().T\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "# Checking for correlations\n",
    "\n",
    "# Which columns might be correlated:\n",
    "# 1.Annual_Income - Monthly_Inhand_Salary\n",
    "# 2.Num_Bank_Accounts - Num_Credit_Card\n",
    "\n",
    "columns_to_analyze_1 = ['Annual_Income','Monthly_Inhand_Salary']\n",
    "\n",
    "pearson_corr_1 = train_df[columns_to_analyze_1].corr(method='pearson')\n",
    "spearman_corr_1 = train_df[columns_to_analyze_1].corr(method='spearman')\n",
    "\n",
    "# print(\"Korelacja Pearsona:\")\n",
    "# print(pearson_corr_1)\n",
    "# print('\\n')\n",
    "# print(\"Korelacja Spearmana:\")\n",
    "# print(spearman_corr_1)\n",
    "\n",
    "# print('\\n' *3)\n",
    "\n",
    "columns_to_analyze_2 = ['Num_Bank_Accounts','Num_Credit_Card']\n",
    "\n",
    "pearson_corr_2 = train_df[columns_to_analyze_2].corr(method='pearson')\n",
    "spearman_corr_2 = train_df[columns_to_analyze_2].corr(method='spearman')\n",
    "\n",
    "# print(\"Korelacja Pearsona:\")\n",
    "# print(pearson_corr_2)\n",
    "# print('\\n')\n",
    "# print(\"Korelacja Spearmana:\")\n",
    "# print(spearman_corr_2)\n",
    "\n",
    "# Conclusion\n",
    "# Annual_Income and Monthly_Inhand_Salary are strongly correlated(0.82)#\n",
    "# We should drop one of these columns\n",
    "\n",
    "# Check for all correlations and analyse\n",
    "column_names = train_df.columns.values.tolist()\n",
    "pearson_corr_all = train_df[column_names].corr(method='pearson')\n",
    "spearman_corr_all = train_df[column_names].corr(method='spearman')\n",
    "\n",
    "# print(\"Korelacja Pearsona:\")\n",
    "# print(pearson_corr_all)\n",
    "# print('\\n')\n",
    "# print(\"Korelacja Spearmana:\")\n",
    "# print(spearman_corr_all)\n",
    "\n",
    "# Considerable correlations:\n",
    "#     Annual_Income - Monthly_Balance 0.66\n",
    "#     Num_Bank_Accounts - Credit_Mix -0.62 credit mix is categorical\n",
    "#     Interest_Rade - Credit_Mix -0.63  credit mix is categorical\n",
    "#     Num_of_Loan - Outstanding_Debt 0.61 not connected in my opinion\n",
    "#     Delay_from_due_date - Credit_Mix -0.61  credit mix is categorical\n",
    "# \n",
    "\n",
    "# Conclusion\n",
    "# Annual_Income and Monthly_Inhand_Salary are strongly correlated(0.82)#\n",
    "# We should drop one of these columns\n",
    "\n",
    "\n",
    "# as annual_income is strongly correlated with monthly balance but monthly inhand salary is not \n",
    "# we will drop annual income column\n",
    "\n",
    "train_df = train_df.drop(\"Annual_Income\", axis='columns')\n",
    "test_df = test_df.drop(\"Annual_Income\", axis='columns')\n",
    "validation_df = validation_df.drop(\"Annual_Income\", axis='columns')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "train_df.columns\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# split data\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # train/test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train_df.drop('Credit_Score', axis=1), train_df.Credit_Score, test_size=0.3, random_state=42)\n",
    "\n",
    "# # train/valid\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# X_valid = X_valid.drop(['Name', 'ID', 'Customer_ID', 'SSN'], axis=1)\n",
    "# # TODO\n",
    "# # pick which columns should be used\n",
    "\n",
    "# # choose a few models and compare them (use score function)\n",
    "\n",
    "X_train = train_df\n",
    "X_test = test_df\n",
    "X_val = validation_df\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "\n",
    "X = pd.concat([X_train, X_val], ignore_index=True)\n",
    "y = pd.concat([y_train, y_val], ignore_index=True)\n",
    "\n",
    "X = X.drop('Month', axis=1)\n",
    "X_test = X_test.drop('Month', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>kind</th>\n",
       "      <th>acc</th>\n",
       "      <th>pre</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>train</td>\n",
       "      <td>0.633658</td>\n",
       "      <td>0.612928</td>\n",
       "      <td>0.583094</td>\n",
       "      <td>0.593050</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>test</td>\n",
       "      <td>0.634429</td>\n",
       "      <td>0.615261</td>\n",
       "      <td>0.585420</td>\n",
       "      <td>0.594905</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>train</td>\n",
       "      <td>0.802304</td>\n",
       "      <td>0.791215</td>\n",
       "      <td>0.787111</td>\n",
       "      <td>0.789126</td>\n",
       "      <td>n_estimators=1000, max_depth=4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>test</td>\n",
       "      <td>0.748857</td>\n",
       "      <td>0.731316</td>\n",
       "      <td>0.727420</td>\n",
       "      <td>0.729306</td>\n",
       "      <td>n_estimators=1000, max_depth=4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>train</td>\n",
       "      <td>0.722038</td>\n",
       "      <td>0.703491</td>\n",
       "      <td>0.685955</td>\n",
       "      <td>0.693570</td>\n",
       "      <td>max_depth=9, min_samples_split=6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>test</td>\n",
       "      <td>0.703714</td>\n",
       "      <td>0.683474</td>\n",
       "      <td>0.667712</td>\n",
       "      <td>0.674500</td>\n",
       "      <td>max_depth=9, min_samples_split=6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name   kind       acc       pre       rec        f1   \n",
       "0  Logistic Regression  train  0.633658  0.612928  0.583094  0.593050  \\\n",
       "1  Logistic Regression   test  0.634429  0.615261  0.585420  0.594905   \n",
       "2     GradientBoosting  train  0.802304  0.791215  0.787111  0.789126   \n",
       "3     GradientBoosting   test  0.748857  0.731316  0.727420  0.729306   \n",
       "4         DecisionTree  train  0.722038  0.703491  0.685955  0.693570   \n",
       "5         DecisionTree   test  0.703714  0.683474  0.667712  0.674500   \n",
       "\n",
       "                             params  \n",
       "0                               NaN  \n",
       "1                               NaN  \n",
       "2    n_estimators=1000, max_depth=4  \n",
       "3    n_estimators=1000, max_depth=4  \n",
       "4  max_depth=9, min_samples_split=6  \n",
       "5  max_depth=9, min_samples_split=6  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# data = {'name': [\"\"],\n",
    "#         'kind': [\"\"],\n",
    "#         'acc': [0],\n",
    "#         'pre': [0],\n",
    "#         'rec': [0],\n",
    "#         'f1': [0],\n",
    "#         'params': [\"\"]}\n",
    "# df = pd.DataFrame(data)\n",
    "# df.to_csv('scores.csv',index=False)\n",
    "\n",
    "data = pd.read_csv(\"scores.csv\")   \n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "\n",
    "# data = pd.read_csv(\"scores.csv\")   \n",
    "# df = pd.DataFrame(data)\n",
    "# df = df.drop(len(df)-1)\n",
    "# df = df.drop('Unnamed: 0', axis=1)\n",
    "# df.to_csv('scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "def save_results(name, kind, acc, pre, rec, f1, params):\n",
    "    data = pd.read_csv(\"scores.csv\")   \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df.loc[len(df)] = {'name': name,\n",
    "                       'kind': kind,\n",
    "                        'acc': acc,\n",
    "                        'pre': pre,\n",
    "                        'rec': rec,\n",
    "                        'f1':  f1,\n",
    "                        'params': params}\n",
    "    \n",
    "    df.to_csv('scores.csv', index=False)\n",
    "    df.head(10)\n",
    "\n",
    "def show_scores(y_test,y_test_pred, name, kind, params=\"\"):\n",
    "\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "    test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "    print(name)\n",
    "    print(\"Accuracy: \", accuracy_score(y_test, y_test_pred))\n",
    "    print(\"Precision: \", precision_score(y_test, y_test_pred, average='macro'))\n",
    "    print(\"Recall: \", recall_score(y_test, y_test_pred, average='macro'))\n",
    "    print(\"F1 score: \", f1_score(y_test, y_test_pred, average='macro'))\n",
    "\n",
    "    save_results(name, kind, test_accuracy, test_precision, test_recall, test_f1, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree\n",
      "Accuracy:  0.7220379746835444\n",
      "Precision:  0.7034907645196519\n",
      "Recall:  0.6859550239259931\n",
      "F1 score:  0.6935701665126744\n",
      "DecisionTree\n",
      "Accuracy:  0.7037142857142857\n",
      "Precision:  0.6834739719886743\n",
      "Recall:  0.6677123268458481\n",
      "F1 score:  0.6745004764379968\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Checking feature importances\n",
    "\n",
    "# dt = DecisionTreeClassifier()\n",
    "# dt.fit(X, y)\n",
    "\n",
    "# feature_importances = dt.feature_importances_\n",
    "# for i in range(len(X.columns)):\n",
    "#     if feature_importances[i] > 0.04:\n",
    "#         print(f\"Importance of feature: {X.columns[i]}: {feature_importances[i]}\")\n",
    "\n",
    "\n",
    "# Dropping features that have too little importance\n",
    "\n",
    "# X_dropped = X\n",
    "# X_test_dropped = X_test\n",
    "\n",
    "# for i in range(len(X.columns)):\n",
    "#     if feature_importances[i] < 0.04: # 0.01, 0.03\n",
    "        \n",
    "#         X_dropped = X_dropped.drop(X.columns[i], axis=1)\n",
    "#         X_test_dropped = X_test_dropped.drop(X.columns[i], axis=1)\n",
    "\n",
    "\n",
    "# Checking default parameters\n",
    "\n",
    "# print(dt.get_params())\n",
    "\n",
    "\n",
    "# Checking results of a model with special parameters\n",
    "\n",
    "# dt = DecisionTreeClassifier(random_state=0, max_depth=9, min_samples_split=6)\n",
    "# dt.fit(X,y)\n",
    "\n",
    "# y_pred = dt.predict(X)\n",
    "# y_test_pred = dt.predict(X_test)\n",
    "\n",
    "# # Showing results and saving it to a df\n",
    "# show_scores(y, y_pred, \"DecisionTree\", \"train\", \"max_depth=9, min_samples_split=6\")\n",
    "# show_scores(y_test, y_test_pred, \"DecisionTree\", \"test\", \"max_depth=9, min_samples_split=6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest\n",
      "Accuracy:  0.7270126582278481\n",
      "Precision:  0.7064304732841569\n",
      "Recall:  0.7088071506956921\n",
      "F1 score:  0.7058214948052246\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [21000, 79000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39m# Showing results and saving it to a df\u001b[39;00m\n\u001b[0;32m     41\u001b[0m show_scores(y, y_pred, \u001b[39m\"\u001b[39m\u001b[39mRandomForest\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmax_depth=9, min_samples_split=6, n_estimators=10000\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m show_scores(y_test, y_test_pred, \u001b[39m\"\u001b[39;49m\u001b[39mRandomForest\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmax_depth=9, min_samples_split=6, n_estimators=10000\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     44\u001b[0m \u001b[39m# RandomForest\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m# Accuracy:  0.8365443037974684\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39m# Precision:  0.8242296165470635\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m# Recall:  0.7171206138429764\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m# F1 score:  0.7161085391536292\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m, in \u001b[0;36mshow_scores\u001b[1;34m(y_test, y_test_pred, name, kind, params)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_scores\u001b[39m(y_test,y_test_pred, name, kind, params\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> 21\u001b[0m     test_accuracy \u001b[39m=\u001b[39m accuracy_score(y_test, y_test_pred)\n\u001b[0;32m     22\u001b[0m     test_precision \u001b[39m=\u001b[39m precision_score(y_test, y_test_pred, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m     test_recall \u001b[39m=\u001b[39m recall_score(y_test, y_test_pred, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mateu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:192\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m validate_parameter_constraints(\n\u001b[0;32m    188\u001b[0m     parameter_constraints, params, caller_name\u001b[39m=\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[0;32m    189\u001b[0m )\n\u001b[0;32m    191\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    194\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    199\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    200\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    201\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    202\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mateu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:221\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[0;32m    157\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    222\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    223\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\mateu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     60\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[39m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m     87\u001b[0m     type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m     type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mateu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [21000, 79000]"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Checking feature importances\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X, y)\n",
    "\n",
    "feature_importances = rf.feature_importances_\n",
    "# for i in range(len(X.columns)):\n",
    "#     if feature_importances[i] > 0.04:\n",
    "#         print(f\"Importance of feature: {X.columns[i]}: {feature_importances[i]}\")\n",
    "\n",
    "\n",
    "# Dropping features that have too little importance\n",
    "\n",
    "# X_dropped = X\n",
    "# X_test_dropped = X_test\n",
    "\n",
    "# for i in range(len(X.columns)):\n",
    "#     if feature_importances[i] < 0.04: # 0.01, 0.03\n",
    "        \n",
    "#         X_dropped = X_dropped.drop(X.columns[i], axis=1)\n",
    "#         X_test_dropped = X_test_dropped.drop(X.columns[i], axis=1)\n",
    "\n",
    "\n",
    "# Checking default parameters\n",
    "\n",
    "# print('\\n', rf.get_params(), '\\n')\n",
    "\n",
    "\n",
    "# Checking results of a model with special parameters\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0, max_depth=9, min_samples_split=6, n_estimators=10000)\n",
    "rf.fit(X,y)\n",
    "\n",
    "y_pred = rf.predict(X)\n",
    "y_test_pred = rf.predict(X_test)\n",
    "\n",
    "# Showing results and saving it to a df\n",
    "show_scores(y, y_pred, \"RandomForest\", \"train\", \"max_depth=9, min_samples_split=6, n_estimators=10000\")\n",
    "show_scores(y_test, y_test_pred, \"RandomForest\", \"test\", \"max_depth=9, min_samples_split=6, n_estimators=10000\")\n",
    "\n",
    "# RandomForest\n",
    "# Accuracy:  0.8365443037974684\n",
    "# Precision:  0.8242296165470635\n",
    "# Recall:  0.8210398665038481\n",
    "# F1 score:  0.822569853743904\n",
    "# RandomForest\n",
    "# Accuracy:  0.7377142857142858\n",
    "# Precision:  0.7162031762195706\n",
    "# Recall:  0.7171206138429764\n",
    "# F1 score:  0.7161085391536292"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
